{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.image as image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path):\n",
    "    # return array of images\n",
    "\n",
    "    labledDataSet = np.empty((1, 10305))\n",
    "    personList = listdir(path)\n",
    "    \n",
    "    for person in personList:\n",
    "        imgList = listdir(path + \"/\" + person)\n",
    "        \n",
    "        for img in imgList:\n",
    "            anImage = image.imread(path + \"/\" + person + \"/\" + img)\n",
    "            #print(path + \"/\" + person + \"==>\" + img)\n",
    "            label = [person]\n",
    "            anImage = anImage.flatten()\n",
    "            labledSample = np.concatenate((label, anImage))\n",
    "            labledSample = labledSample.reshape(1, 10305)\n",
    "            labledDataSet = np.append(labledDataSet, labledSample, 0)\n",
    "    \n",
    "    labledDataSet = labledDataSet[1:,:]\n",
    "    return labledDataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"att_faces/orl_faces\"\n",
    "\n",
    "#load the dataset\n",
    "labeledData = loadImages(path)\n",
    "\n",
    "#split the dataset\n",
    "trainingLabeledData = labeledData[1::2]\n",
    "testLabeledData = labeledData[0::2]\n",
    "\n",
    "#split the labels from the data\n",
    "trainingLabels, trainingData = trainingLabeledData[:,0], trainingLabeledData[:,1:]\n",
    "trainingData = np.array(trainingData).astype(np.float64)\n",
    "\n",
    "#scale the training data\n",
    "scaledTrainingData = preprocessing.scale(trainingData)\n",
    "\n",
    "#provide an access to each label\n",
    "trainingDict = {trainingLabel: trainingData[trainingLabels == trainingLabel] for trainingLabel in np.unique(trainingLabels)}\n",
    "\n",
    "testLabels, testData = testLabeledData[:,0], testLabeledData[:,1:]\n",
    "testData = np.array(testData).astype(np.float64)\n",
    "\n",
    "#scale the test data\n",
    "scaledTestData = preprocessing.scale(testData)\n",
    "\n",
    "#provide an access to each label\n",
    "testDict = {testLabel: testData[testLabels == testLabel] for testLabel in np.unique(testLabels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the required means (the total mean should be close to zero)\n",
    "totalMean = np.matrix(np.mean(scaledTrainingData, axis = 0))\n",
    "\n",
    "classMeans = dict()\n",
    "\n",
    "for label, labelData in trainingDict.items():\n",
    "    classMeans[label] = np.matrix(np.mean(labelData, axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the between-class matrix Sb and the within-class matrix Sw \n",
    "Sb = np.zeros((10304, 10304))\n",
    "Sw = np.zeros((10304, 10304))\n",
    "\n",
    "for label, classMean in classMeans.items():\n",
    "    diffBetween = classMean - totalMean\n",
    "    Sb += np.ma.size(trainingDict[label], 0) * np.dot(diffBetween.T, diffBetween)\n",
    "    \n",
    "    diffWithin = trainingDict[label] - classMean\n",
    "    Sw += np.dot(diffWithin.T, diffWithin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the pseudoinverse of Sw then SwPinv . Sb\n",
    "SwPinv = np.linalg.pinv(Sw)\n",
    "x = np.dot(SwPinv, Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the eigen vectors of SwPinv . Sb\n",
    "[eigenVals, eigenVects] = np.linalg.eig(x)\n",
    "\n",
    "idx = np.argsort(eigenVals)\n",
    "idx = np.flip(idx)\n",
    "eigenVals = eigenVals[idx]\n",
    "eigenVects = eigenVects[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error distance for eigen values if we ommit the complex part\n",
      "0.22764755425863617\n"
     ]
    }
   ],
   "source": [
    "realEigenVals = eigenVals.real\n",
    "realEigenVects = eigenVects.real\n",
    "print(\"error distance for eigen values if we ommit the complex part\")\n",
    "print(np.linalg.norm(eigenVals - realEigenVals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor for Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One nearest neighbor\n",
      "[[3 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          s1       1.00      0.60      0.75         5\n",
      "         s10       1.00      1.00      1.00         5\n",
      "         s11       1.00      1.00      1.00         5\n",
      "         s12       1.00      1.00      1.00         5\n",
      "         s13       0.83      1.00      0.91         5\n",
      "         s14       1.00      1.00      1.00         5\n",
      "         s15       1.00      1.00      1.00         5\n",
      "         s16       1.00      0.80      0.89         5\n",
      "         s17       0.71      1.00      0.83         5\n",
      "         s18       1.00      0.60      0.75         5\n",
      "         s19       1.00      1.00      1.00         5\n",
      "          s2       0.83      1.00      0.91         5\n",
      "         s20       1.00      0.80      0.89         5\n",
      "         s21       1.00      1.00      1.00         5\n",
      "         s22       0.83      1.00      0.91         5\n",
      "         s23       1.00      0.60      0.75         5\n",
      "         s24       1.00      0.80      0.89         5\n",
      "         s25       0.83      1.00      0.91         5\n",
      "         s26       1.00      1.00      1.00         5\n",
      "         s27       1.00      1.00      1.00         5\n",
      "         s28       1.00      0.80      0.89         5\n",
      "         s29       0.83      1.00      0.91         5\n",
      "          s3       0.71      1.00      0.83         5\n",
      "         s30       0.71      1.00      0.83         5\n",
      "         s31       1.00      1.00      1.00         5\n",
      "         s32       1.00      1.00      1.00         5\n",
      "         s33       1.00      1.00      1.00         5\n",
      "         s34       0.83      1.00      0.91         5\n",
      "         s35       1.00      0.80      0.89         5\n",
      "         s36       1.00      0.80      0.89         5\n",
      "         s37       0.83      1.00      0.91         5\n",
      "         s38       1.00      1.00      1.00         5\n",
      "         s39       1.00      1.00      1.00         5\n",
      "          s4       1.00      0.80      0.89         5\n",
      "         s40       1.00      0.40      0.57         5\n",
      "          s5       0.71      1.00      0.83         5\n",
      "          s6       1.00      1.00      1.00         5\n",
      "          s7       0.83      1.00      0.91         5\n",
      "          s8       1.00      1.00      1.00         5\n",
      "          s9       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       200\n",
      "   macro avg       0.94      0.92      0.92       200\n",
      "weighted avg       0.94      0.92      0.92       200\n",
      "\n",
      "accuracy score\n",
      "0.92\n",
      "============================================================\n",
      "Three nearest neighbors\n",
      "[[3 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          s1       1.00      0.60      0.75         5\n",
      "         s10       1.00      1.00      1.00         5\n",
      "         s11       1.00      1.00      1.00         5\n",
      "         s12       1.00      1.00      1.00         5\n",
      "         s13       0.83      1.00      0.91         5\n",
      "         s14       1.00      1.00      1.00         5\n",
      "         s15       1.00      1.00      1.00         5\n",
      "         s16       0.80      0.80      0.80         5\n",
      "         s17       0.62      1.00      0.77         5\n",
      "         s18       0.75      0.60      0.67         5\n",
      "         s19       1.00      1.00      1.00         5\n",
      "          s2       1.00      1.00      1.00         5\n",
      "         s20       1.00      1.00      1.00         5\n",
      "         s21       0.62      1.00      0.77         5\n",
      "         s22       0.50      1.00      0.67         5\n",
      "         s23       1.00      0.60      0.75         5\n",
      "         s24       1.00      1.00      1.00         5\n",
      "         s25       1.00      0.60      0.75         5\n",
      "         s26       1.00      1.00      1.00         5\n",
      "         s27       1.00      1.00      1.00         5\n",
      "         s28       0.80      0.80      0.80         5\n",
      "         s29       0.83      1.00      0.91         5\n",
      "          s3       0.71      1.00      0.83         5\n",
      "         s30       0.83      1.00      0.91         5\n",
      "         s31       1.00      0.60      0.75         5\n",
      "         s32       1.00      1.00      1.00         5\n",
      "         s33       1.00      0.80      0.89         5\n",
      "         s34       0.83      1.00      0.91         5\n",
      "         s35       0.00      0.00      0.00         5\n",
      "         s36       1.00      0.60      0.75         5\n",
      "         s37       0.80      0.80      0.80         5\n",
      "         s38       1.00      1.00      1.00         5\n",
      "         s39       1.00      1.00      1.00         5\n",
      "          s4       1.00      1.00      1.00         5\n",
      "         s40       1.00      0.40      0.57         5\n",
      "          s5       0.83      1.00      0.91         5\n",
      "          s6       1.00      1.00      1.00         5\n",
      "          s7       0.71      1.00      0.83         5\n",
      "          s8       1.00      1.00      1.00         5\n",
      "          s9       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       200\n",
      "   macro avg       0.89      0.88      0.87       200\n",
      "weighted avg       0.89      0.88      0.87       200\n",
      "\n",
      "accuracy score\n",
      "0.88\n",
      "============================================================\n",
      "Five nearest neighbors\n",
      "[[3 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 1 0 ... 0 4 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          s1       1.00      0.60      0.75         5\n",
      "         s10       0.83      1.00      0.91         5\n",
      "         s11       1.00      1.00      1.00         5\n",
      "         s12       1.00      1.00      1.00         5\n",
      "         s13       1.00      1.00      1.00         5\n",
      "         s14       1.00      1.00      1.00         5\n",
      "         s15       1.00      1.00      1.00         5\n",
      "         s16       0.80      0.80      0.80         5\n",
      "         s17       0.71      1.00      0.83         5\n",
      "         s18       0.80      0.80      0.80         5\n",
      "         s19       1.00      1.00      1.00         5\n",
      "          s2       1.00      1.00      1.00         5\n",
      "         s20       1.00      1.00      1.00         5\n",
      "         s21       0.62      1.00      0.77         5\n",
      "         s22       0.42      1.00      0.59         5\n",
      "         s23       1.00      0.60      0.75         5\n",
      "         s24       1.00      1.00      1.00         5\n",
      "         s25       1.00      0.80      0.89         5\n",
      "         s26       1.00      1.00      1.00         5\n",
      "         s27       0.83      1.00      0.91         5\n",
      "         s28       0.80      0.80      0.80         5\n",
      "         s29       1.00      1.00      1.00         5\n",
      "          s3       0.80      0.80      0.80         5\n",
      "         s30       0.83      1.00      0.91         5\n",
      "         s31       1.00      0.60      0.75         5\n",
      "         s32       1.00      1.00      1.00         5\n",
      "         s33       1.00      0.80      0.89         5\n",
      "         s34       0.71      1.00      0.83         5\n",
      "         s35       0.00      0.00      0.00         5\n",
      "         s36       1.00      0.80      0.89         5\n",
      "         s37       0.80      0.80      0.80         5\n",
      "         s38       1.00      1.00      1.00         5\n",
      "         s39       1.00      1.00      1.00         5\n",
      "          s4       1.00      1.00      1.00         5\n",
      "         s40       0.71      1.00      0.83         5\n",
      "          s5       1.00      0.60      0.75         5\n",
      "          s6       1.00      1.00      1.00         5\n",
      "          s7       0.83      1.00      0.91         5\n",
      "          s8       1.00      0.80      0.89         5\n",
      "          s9       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       200\n",
      "   macro avg       0.89      0.88      0.87       200\n",
      "weighted avg       0.89      0.88      0.87       200\n",
      "\n",
      "accuracy score\n",
      "0.875\n",
      "============================================================\n",
      "Seven nearest neighbors\n",
      "[[3 0 0 ... 0 0 0]\n",
      " [0 4 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          s1       1.00      0.60      0.75         5\n",
      "         s10       1.00      0.80      0.89         5\n",
      "         s11       1.00      1.00      1.00         5\n",
      "         s12       1.00      1.00      1.00         5\n",
      "         s13       1.00      1.00      1.00         5\n",
      "         s14       1.00      1.00      1.00         5\n",
      "         s15       0.83      1.00      0.91         5\n",
      "         s16       0.67      0.80      0.73         5\n",
      "         s17       0.83      1.00      0.91         5\n",
      "         s18       0.80      0.80      0.80         5\n",
      "         s19       1.00      1.00      1.00         5\n",
      "          s2       1.00      1.00      1.00         5\n",
      "         s20       1.00      1.00      1.00         5\n",
      "         s21       0.71      1.00      0.83         5\n",
      "         s22       0.36      1.00      0.53         5\n",
      "         s23       1.00      1.00      1.00         5\n",
      "         s24       1.00      1.00      1.00         5\n",
      "         s25       1.00      0.80      0.89         5\n",
      "         s26       1.00      1.00      1.00         5\n",
      "         s27       0.83      1.00      0.91         5\n",
      "         s28       0.67      0.40      0.50         5\n",
      "         s29       0.83      1.00      0.91         5\n",
      "          s3       0.67      0.80      0.73         5\n",
      "         s30       0.80      0.80      0.80         5\n",
      "         s31       1.00      0.80      0.89         5\n",
      "         s32       1.00      1.00      1.00         5\n",
      "         s33       1.00      0.80      0.89         5\n",
      "         s34       0.83      1.00      0.91         5\n",
      "         s35       0.00      0.00      0.00         5\n",
      "         s36       1.00      0.80      0.89         5\n",
      "         s37       0.57      0.80      0.67         5\n",
      "         s38       1.00      1.00      1.00         5\n",
      "         s39       1.00      0.60      0.75         5\n",
      "          s4       1.00      1.00      1.00         5\n",
      "         s40       0.50      0.60      0.55         5\n",
      "          s5       1.00      0.40      0.57         5\n",
      "          s6       1.00      1.00      1.00         5\n",
      "          s7       0.83      1.00      0.91         5\n",
      "          s8       1.00      1.00      1.00         5\n",
      "          s9       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       200\n",
      "   macro avg       0.87      0.85      0.84       200\n",
      "weighted avg       0.87      0.85      0.84       200\n",
      "\n",
      "accuracy score\n",
      "0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda\\envs\\my-env\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  \n",
    "\n",
    "#project the training data\n",
    "topK = realEigenVects[:, :39]\n",
    "projectedTrainingData = np.dot(trainingData, topK)\n",
    "\n",
    "#project the test data\n",
    "projectedTestData = np.dot(testData, topK)\n",
    "\n",
    "#train the classifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 1)\n",
    "classifierThree = KNeighborsClassifier(n_neighbors = 3)\n",
    "classifierFive = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifierSeven = KNeighborsClassifier(n_neighbors = 7)\n",
    "\n",
    "classifier.fit(projectedTrainingData, trainingLabels)\n",
    "classifierThree.fit(projectedTrainingData, trainingLabels) \n",
    "classifierFive.fit(projectedTrainingData, trainingLabels) \n",
    "classifierSeven.fit(projectedTrainingData, trainingLabels) \n",
    "\n",
    "#predict\n",
    "labelPredict = classifier.predict(projectedTestData)\n",
    "labelPredictThree = classifierThree.predict(projectedTestData)\n",
    "labelPredictFive = classifierFive.predict(projectedTestData)\n",
    "labelPredictSeven = classifierSeven.predict(projectedTestData)\n",
    "\n",
    "#evaluation\n",
    "print(\"One nearest neighbor\")\n",
    "print(confusion_matrix(testLabels, labelPredict))  \n",
    "print(classification_report(testLabels, labelPredict))\n",
    "print(\"accuracy score\")\n",
    "accuracyOne = accuracy_score(testLabels, labelPredict)\n",
    "print(accuracyOne)\n",
    "print(\"============================================================\")\n",
    "print(\"Three nearest neighbors\")\n",
    "print(confusion_matrix(testLabels, labelPredictThree))  \n",
    "print(classification_report(testLabels, labelPredictThree))\n",
    "print(\"accuracy score\")\n",
    "accuracyThree = accuracy_score(testLabels, labelPredictThree)\n",
    "print(accuracyThree)\n",
    "print(\"============================================================\")\n",
    "print(\"Five nearest neighbors\")\n",
    "print(confusion_matrix(testLabels, labelPredictFive))  \n",
    "print(classification_report(testLabels, labelPredictFive))\n",
    "print(\"accuracy score\")\n",
    "accuracyFive = accuracy_score(testLabels, labelPredictFive)\n",
    "print(accuracyFive)\n",
    "print(\"============================================================\")\n",
    "print(\"Seven nearest neighbors\")\n",
    "print(confusion_matrix(testLabels, labelPredictSeven))  \n",
    "print(classification_report(testLabels, labelPredictSeven))\n",
    "print(\"accuracy score\")\n",
    "accuracySeven = accuracy_score(testLabels, labelPredictSeven)\n",
    "print(accuracySeven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1768fbe8780>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "xline = [1, 3, 5, 7]\n",
    "yline = [accuracyOne, accuracyThree, accuracyFive, accuracySeven]\n",
    "plt.plot(xline, yline, \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetDic = {label: labeledData[labeledData[:,0] == label][:,1:] for label in np.unique(labeledData[:,0])}\n",
    "y_test = np.empty((2,1))\n",
    "y_train= np.empty((2,1))\n",
    "x_train = np.empty((1,10304))\n",
    "x_test = np.empty((1,10304))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 10304)\n",
      "(280, 10304)\n",
      "(120, 10304)\n",
      "(280, 1)\n",
      "(120, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(41):\n",
    "    \n",
    "    if(i != 0):\n",
    "        label = 's'+str(i)\n",
    "        x_train = np.vstack((x_train,dataSetDic[label][0:7,:]))\n",
    "        x_test = np.vstack((x_test,dataSetDic[label][7:,:]))\n",
    "        \n",
    "        for count in range(7):\n",
    "            y_train = np.vstack((y_train,[label]))\n",
    "            \n",
    "        for index in range(3):\n",
    "            y_test = np.vstack((y_test,[label]))\n",
    "\n",
    "y_train = y_train[2: , :]\n",
    "x_train = x_train[1: , :]\n",
    "x_train = x_train.astype(float)\n",
    "x_trainDict = {label: x_train[y_train.reshape(280,) == label] for label in np.unique(y_train)}\n",
    "\n",
    "\n",
    "\n",
    "y_test = y_test[2: , :]\n",
    "x_test= x_test[1: , :]\n",
    "x_test = x_test.astype(float)\n",
    "print(x_test.shape)\n",
    "x_testDict = {test_label:x_test[y_test.reshape(120,)==test_label] for test_label in np.unique(y_test)}\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10304)\n"
     ]
    }
   ],
   "source": [
    "#total mean of all classes\n",
    "totalMean = np.matrix(np.mean(x_train,0))\n",
    "print(totalMean.shape)\n",
    "#mean for each class\n",
    "classMean = dict()\n",
    "for label, data in x_trainDict.items():\n",
    "    classMean[label] = np.matrix(np.mean(data,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sb = np.zeros((10304,10304))\n",
    "Sw = np.zeros((10304,10304))\n",
    "\n",
    "for label, calss_mean in classMean.items():\n",
    "    diff = calss_mean - totalMean\n",
    "    Sb += x_trainDict[label].size * np.dot(diff.T,diff)\n",
    "    \n",
    "    diffWithin = x_trainDict[label] - calss_mean\n",
    "    Sw += np.dot(diffWithin.T, diffWithin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get inverse of within matrix\n",
    "SwPinv = np.linalg.pinv(Sw)\n",
    "S =np.dot(SwPinv, Sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get eigenVectors and eigenValues of S\n",
    "eigenValues_b, eigenVectors_b = np.linalg.eig(S)\n",
    "\n",
    "#descending order\n",
    "idx = np.argsort(eigenValues_b)[::-1]\n",
    "eigenVectors_b = eigenVectors_b[:,idx]\n",
    " # sort eigenvectors according to same index\n",
    "eigenValues_b = eigenValues_b[idx]\n",
    "\n",
    "realEigenVals_b = eigenValues_b.real\n",
    "realEigenVects_b = eigenVectors_b.real\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8916666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda\\envs\\my-env\\lib\\site-packages\\ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Classification using K-NN\n",
    "\n",
    "reduced_realEigenVects_b = realEigenVects_b[:, 0:39]\n",
    "projected_trainingSet = np.dot(x_train, reduced_realEigenVects_b)\n",
    "projected_testingSet = np.dot(x_test, reduced_realEigenVects_b)\n",
    "\n",
    "classifer_b = KNeighborsClassifier(n_neighbors= 1)\n",
    "classifer_b.fit(projected_trainingSet, y_train)\n",
    "\n",
    "prediction = classifer_b.predict(projected_testingSet)\n",
    "accuracy = accuracy_score(prediction, y_test)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
